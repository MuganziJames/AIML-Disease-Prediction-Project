{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Heart Disease Prediction - Model Training & Evaluation\n",
        "\n",
        "This notebook implements the complete machine learning pipeline for heart disease prediction, following the plan structure:\n",
        "\n",
        "## Table of Contents\n",
        "1. Setup & Data Loading\n",
        "2. Data Preprocessing \n",
        "3. Model Training (Logistic Regression & Random Forest)\n",
        "4. Hyperparameter Tuning\n",
        "5. Model Evaluation & Comparison\n",
        "6. Model Persistence\n",
        "7. Demo Predictions\n",
        "\n",
        "**Goal**: Build accurate models to predict heart disease risk and save the best performing model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All libraries imported successfully!\n",
            "Pandas: 2.2.3\n",
            "NumPy: 2.1.3\n",
            "Scikit-learn imported\n"
          ]
        }
      ],
      "source": [
        "# 0) Setup - Import libraries and configure environment\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('../src')\n",
        "\n",
        "# Scikit-learn imports\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, ConfusionMatrixDisplay, \n",
        "                             RocCurveDisplay, classification_report)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import joblib\n",
        "\n",
        "# Local utilities\n",
        "from utils import load_and_create_target, plot_confusion_matrix, plot_roc_curve, print_model_metrics\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"Pandas: {pd.__version__}\")\n",
        "print(f\"NumPy: {np.__version__}\")\n",
        "print(f\"Scikit-learn imported\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading & Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading heart disease dataset...\n",
            "Dataset loaded successfully!\n",
            "Shape: (5, 18)\n",
            "Target distribution: {0: 3, 1: 2}\n",
            "\n",
            "Features: ['age', 'trestbps', 'chol', 'fbs', 'restecg', 'thalch', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'sex_Female', 'sex_Male', 'cp_asymptomatic', 'cp_atypical angina', 'cp_non-anginal', 'cp_typical angina']\n",
            "Feature count: 17\n",
            "Sample count: 5\n",
            "\n",
            "First few samples:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalch</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>sex_Female</th>\n",
              "      <th>sex_Male</th>\n",
              "      <th>cp_asymptomatic</th>\n",
              "      <th>cp_atypical angina</th>\n",
              "      <th>cp_non-anginal</th>\n",
              "      <th>cp_typical angina</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>58</td>\n",
              "      <td>130</td>\n",
              "      <td>220</td>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "      <td>150</td>\n",
              "      <td>False</td>\n",
              "      <td>1.4</td>\n",
              "      <td>flat</td>\n",
              "      <td>0</td>\n",
              "      <td>fixed defect</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>67</td>\n",
              "      <td>160</td>\n",
              "      <td>276</td>\n",
              "      <td>0</td>\n",
              "      <td>lv hypertrophy</td>\n",
              "      <td>108</td>\n",
              "      <td>True</td>\n",
              "      <td>1.5</td>\n",
              "      <td>flat</td>\n",
              "      <td>3</td>\n",
              "      <td>normal</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>42</td>\n",
              "      <td>120</td>\n",
              "      <td>230</td>\n",
              "      <td>0</td>\n",
              "      <td>normal</td>\n",
              "      <td>170</td>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>upsloping</td>\n",
              "      <td>0</td>\n",
              "      <td>reversable defect</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>50</td>\n",
              "      <td>130</td>\n",
              "      <td>210</td>\n",
              "      <td>0</td>\n",
              "      <td>lv hypertrophy</td>\n",
              "      <td>158</td>\n",
              "      <td>False</td>\n",
              "      <td>0.8</td>\n",
              "      <td>flat</td>\n",
              "      <td>0</td>\n",
              "      <td>normal</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>45</td>\n",
              "      <td>114</td>\n",
              "      <td>230</td>\n",
              "      <td>0</td>\n",
              "      <td>normal</td>\n",
              "      <td>165</td>\n",
              "      <td>False</td>\n",
              "      <td>1.1</td>\n",
              "      <td>downsloping</td>\n",
              "      <td>0</td>\n",
              "      <td>normal</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  trestbps  chol  fbs         restecg  thalch  exang  oldpeak  \\\n",
              "0   58       130   220    1          normal     150  False      1.4   \n",
              "1   67       160   276    0  lv hypertrophy     108   True      1.5   \n",
              "2   42       120   230    0          normal     170  False      1.0   \n",
              "3   50       130   210    0  lv hypertrophy     158  False      0.8   \n",
              "4   45       114   230    0          normal     165  False      1.1   \n",
              "\n",
              "         slope  ca               thal  sex_Female  sex_Male  cp_asymptomatic  \\\n",
              "0         flat   0       fixed defect           0         1                0   \n",
              "1         flat   3             normal           0         1                1   \n",
              "2    upsloping   0  reversable defect           1         0                0   \n",
              "3         flat   0             normal           0         1                0   \n",
              "4  downsloping   0             normal           1         0                0   \n",
              "\n",
              "   cp_atypical angina  cp_non-anginal  cp_typical angina  \n",
              "0                   0               0                  1  \n",
              "1                   0               0                  0  \n",
              "2                   0               1                  0  \n",
              "3                   0               1                  0  \n",
              "4                   1               0                  0  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1) Load data with target creation\n",
        "print(\"Loading heart disease dataset...\")\n",
        "df = load_and_create_target('../data/heart_dataset.csv')\n",
        "\n",
        "print(f\"Dataset loaded successfully!\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Target distribution: {df['target'].value_counts().to_dict()}\")\n",
        "\n",
        "# Separate features and target\n",
        "y = df['target'].astype(int)\n",
        "X = df.drop(columns=['target'])\n",
        "\n",
        "print(f\"\\nFeatures: {list(X.columns)}\")\n",
        "print(f\"Feature count: {X.shape[1]}\")\n",
        "print(f\"Sample count: {X.shape[0]}\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nFirst few samples:\")\n",
        "X.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data preprocessing...\n",
            "Missing values: 0\n",
            "Numeric features (13): ['age', 'trestbps', 'chol', 'fbs', 'thalch', 'oldpeak', 'ca', 'sex_Female', 'sex_Male', 'cp_asymptomatic', 'cp_atypical angina', 'cp_non-anginal', 'cp_typical angina']\n",
            "Categorical features (4): ['restecg', 'exang', 'slope', 'thal']\n",
            "Preprocessor created!\n"
          ]
        }
      ],
      "source": [
        "# 2) Basic data cleaning and feature identification\n",
        "print(\"Data preprocessing...\")\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = X.isnull().sum().sum()\n",
        "print(f\"Missing values: {missing_values}\")\n",
        "\n",
        "# Identify feature types\n",
        "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "cat_cols = [c for c in X.columns if c not in num_cols]\n",
        "\n",
        "print(f\"Numeric features ({len(num_cols)}): {num_cols}\")\n",
        "print(f\"Categorical features ({len(cat_cols)}): {cat_cols}\")\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), num_cols),\n",
        "    ('cat', 'passthrough', cat_cols)  # Keep categorical as-is since they're already encoded\n",
        "], remainder='drop')\n",
        "\n",
        "print(\"Preprocessor created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Train/Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Splitting data into train and test sets...\n",
            "Warning: Small dataset. Using simple split without stratification.\n",
            "Training set: (3, 17)\n",
            "Test set: (2, 17)\n",
            "Training target distribution: {0: 2, 1: 1}\n",
            "Test target distribution: {1: 1, 0: 1}\n"
          ]
        }
      ],
      "source": [
        "# 3) Train-test split\n",
        "print(\"Splitting data into train and test sets...\")\n",
        "\n",
        "# Check if we have enough data for proper split\n",
        "if len(X) >= 10:  # Need enough samples for proper stratified split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, \n",
        "        stratify=y if len(np.unique(y)) > 1 else None\n",
        "    )\n",
        "elif len(X) >= 4:  # Small dataset - no stratification\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.4, random_state=42  # Larger test size, no stratify\n",
        "    )\n",
        "    print(\"Warning: Small dataset. Using simple split without stratification.\")\n",
        "else:\n",
        "    print(\"Warning: Very small dataset. Using all data for both training and testing.\")\n",
        "    X_train, X_test = X, X\n",
        "    y_train, y_test = y, y\n",
        "\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "print(f\"Training target distribution: {y_train.value_counts().to_dict()}\")\n",
        "print(f\"Test target distribution: {y_test.value_counts().to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Baseline Models Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training baseline models...\n",
            "\n",
            "Training LOGISTIC_REGRESSION...\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "could not convert string to float: 'normal'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     19\u001b[39m pipe = Pipeline([\n\u001b[32m     20\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mpreprocessor\u001b[39m\u001b[33m'\u001b[39m, preprocessor), \n\u001b[32m     21\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m'\u001b[39m, clf)\n\u001b[32m     22\u001b[39m ])\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[32m     28\u001b[39m y_pred = pipe.predict(X_test)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py:662\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    657\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    658\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    659\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    660\u001b[39m             all_params=params,\n\u001b[32m    661\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1222\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1219\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1220\u001b[39m     _dtype = [np.float64, np.float32]\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mliblinear\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msag\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msaga\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1231\u001b[39m check_classification_targets(y)\n\u001b[32m   1232\u001b[39m \u001b[38;5;28mself\u001b[39m.classes_ = np.unique(y)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2961\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2959\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2960\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2961\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2962\u001b[39m     out = X, y\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1370\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1364\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1365\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1366\u001b[39m     )\n\u001b[32m   1368\u001b[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m-> \u001b[39m\u001b[32m1370\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1389\u001b[39m check_consistent_length(X, y)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1055\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1053\u001b[39m         array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1054\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1055\u001b[39m         array = \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1056\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1057\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1058\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.format(array)\n\u001b[32m   1059\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcomplex_warning\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:839\u001b[39m, in \u001b[36m_asarray_with_order\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    837\u001b[39m     array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    838\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m839\u001b[39m     array = \u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray(array)\n",
            "\u001b[31mValueError\u001b[39m: could not convert string to float: 'normal'"
          ]
        }
      ],
      "source": [
        "# 4) Define and train baseline models\n",
        "print(\"Training baseline models...\")\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    \"logistic_regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
        "    \"random_forest\": RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "}\n",
        "\n",
        "# Store results\n",
        "results = {}\n",
        "trained_models = {}\n",
        "\n",
        "# Train each model\n",
        "for name, clf in models.items():\n",
        "    print(f\"\\nTraining {name.upper()}...\")\n",
        "    \n",
        "    # Create pipeline\n",
        "    pipe = Pipeline([\n",
        "        ('preprocessor', preprocessor), \n",
        "        ('classifier', clf)\n",
        "    ])\n",
        "    \n",
        "    # Train model\n",
        "    pipe.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = pipe.predict(X_test)\n",
        "    y_proba = pipe.predict_proba(X_test)[:, 1] if hasattr(pipe['classifier'], \"predict_proba\") else None\n",
        "    \n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
        "        \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
        "        \"f1\": f1_score(y_test, y_pred, zero_division=0),\n",
        "        \"roc_auc\": roc_auc_score(y_test, y_proba) if y_proba is not None else None\n",
        "    }\n",
        "    \n",
        "    # Store results\n",
        "    results[name] = metrics\n",
        "    trained_models[name] = pipe\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"{name.upper()} Results:\")\n",
        "    print(f\"   Accuracy: {metrics['accuracy']:.4f}\")\n",
        "    print(f\"   Precision: {metrics['precision']:.4f}\")\n",
        "    print(f\"   Recall: {metrics['recall']:.4f}\")\n",
        "    print(f\"   F1 Score: {metrics['f1']:.4f}\")\n",
        "    if metrics['roc_auc']:\n",
        "        print(f\"   ROC AUC: {metrics['roc_auc']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"BASELINE MODELS TRAINING COMPLETE!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Hyperparameter Tuning (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hyperparameter tuning (if dataset is large enough)...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      5\u001b[39m param_grids = {\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlogistic_regression\u001b[39m\u001b[33m'\u001b[39m: {\n\u001b[32m      7\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mclassifier__C\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m0.1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m10\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     }\n\u001b[32m     16\u001b[39m }\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Only do hyperparameter tuning if we have enough data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mX_train\u001b[49m) > \u001b[32m10\u001b[39m:\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPerforming hyperparameter tuning...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m     tuned_models = {}\n",
            "\u001b[31mNameError\u001b[39m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "# Optional: Hyperparameter tuning for better performance\n",
        "print(\"Hyperparameter tuning (if dataset is large enough)...\")\n",
        "\n",
        "# Define parameter grids\n",
        "param_grids = {\n",
        "    'logistic_regression': {\n",
        "        'classifier__C': [0.1, 1, 10],\n",
        "        'classifier__penalty': ['l2'],\n",
        "        'classifier__solver': ['lbfgs']\n",
        "    },\n",
        "    'random_forest': {\n",
        "        'classifier__n_estimators': [100, 300],\n",
        "        'classifier__max_depth': [None, 5, 10],\n",
        "        'classifier__min_samples_split': [2, 5]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Only do hyperparameter tuning if we have enough data\n",
        "if len(X_train) > 10:\n",
        "    print(\"Performing hyperparameter tuning...\")\n",
        "    \n",
        "    tuned_models = {}\n",
        "    for name, pipe in trained_models.items():\n",
        "        if name in param_grids:\n",
        "            print(f\"\\nTuning {name.upper()}...\")\n",
        "            \n",
        "            grid_search = GridSearchCV(\n",
        "                pipe, \n",
        "                param_grids[name],\n",
        "                cv=min(3, len(X_train)//2),  # Adjust CV folds based on data size\n",
        "                scoring='roc_auc',\n",
        "                n_jobs=-1\n",
        "            )\n",
        "            \n",
        "            grid_search.fit(X_train, y_train)\n",
        "            tuned_models[name] = grid_search.best_estimator_\n",
        "            \n",
        "            print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "            print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
        "    \n",
        "    # Update models if tuning was successful\n",
        "    if tuned_models:\n",
        "        trained_models.update(tuned_models)\n",
        "        print(\"\\nModels updated with best hyperparameters!\")\n",
        "    \n",
        "else:\n",
        "    print(\"Dataset too small for hyperparameter tuning. Using default parameters.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Evaluation & Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== COMPREHENSIVE MODEL EVALUATION ===\n",
            "\n",
            "Model Comparison:\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'f1'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m     best_model_name = results_df[\u001b[33m'\u001b[39m\u001b[33mcombined_score\u001b[39m\u001b[33m'\u001b[39m].idxmax()\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# Sort by F1 score if ROC AUC not available\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     best_model_name = \u001b[43mresults_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mf1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.idxmax()\n\u001b[32m     19\u001b[39m best_model = trained_models[best_model_name]\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBEST MODEL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_name.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\james\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
            "\u001b[31mKeyError\u001b[39m: 'f1'"
          ]
        }
      ],
      "source": [
        "# 5) Comprehensive model evaluation and comparison\n",
        "print(\"=== COMPREHENSIVE MODEL EVALUATION ===\")\n",
        "\n",
        "# Create comparison DataFrame\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"\\nModel Comparison:\")\n",
        "print(results_df.round(4))\n",
        "\n",
        "# Find best model\n",
        "best_model_name = None\n",
        "if 'roc_auc' in results_df.columns:\n",
        "    # Sort by ROC AUC first, then F1\n",
        "    results_df['combined_score'] = results_df['roc_auc'].fillna(0) + results_df['f1']\n",
        "    best_model_name = results_df['combined_score'].idxmax()\n",
        "else:\n",
        "    # Sort by F1 score if ROC AUC not available\n",
        "    best_model_name = results_df['f1'].idxmax()\n",
        "\n",
        "best_model = trained_models[best_model_name]\n",
        "\n",
        "print(f\"\\nBEST MODEL: {best_model_name.upper()}\")\n",
        "print(f\"Performance: {results_df.loc[best_model_name].to_dict()}\")\n",
        "\n",
        "# Get predictions from best model\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "try:\n",
        "    y_proba_best = best_model.predict_proba(X_test)[:, 1]\n",
        "except:\n",
        "    y_proba_best = None\n",
        "\n",
        "print_model_metrics(y_test, y_pred_best, y_proba_best, best_model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating visualizations...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'y_test' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Plot confusion matrix\u001b[39;00m\n\u001b[32m      8\u001b[39m plt.figure(figsize=(\u001b[32m8\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m confusion_fig = plot_confusion_matrix(\u001b[43my_test\u001b[49m, y_pred_best, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConfusion Matrix - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_name.title()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m plt.savefig(\u001b[33m'\u001b[39m\u001b[33m../reports/confusion_matrix.png\u001b[39m\u001b[33m'\u001b[39m, dpi=\u001b[32m300\u001b[39m, bbox_inches=\u001b[33m'\u001b[39m\u001b[33mtight\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m plt.show()\n",
            "\u001b[31mNameError\u001b[39m: name 'y_test' is not defined"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 6) Visualizations - Confusion Matrix and ROC Curve\n",
        "print(\"Creating visualizations...\")\n",
        "\n",
        "# Create figures directory if it doesn't exist\n",
        "os.makedirs('../reports', exist_ok=True)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "confusion_fig = plot_confusion_matrix(y_test, y_pred_best, f\"Confusion Matrix - {best_model_name.title()}\")\n",
        "plt.savefig('../reports/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC curve if probabilities available\n",
        "if y_proba_best is not None:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    roc_fig = plot_roc_curve(y_test, y_proba_best, f\"ROC Curve - {best_model_name.title()}\")\n",
        "    plt.savefig('../reports/roc_curve.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Model comparison chart\n",
        "plt.figure(figsize=(12, 8))\n",
        "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
        "x = np.arange(len(metrics_to_plot))\n",
        "width = 0.35\n",
        "\n",
        "for i, (model_name, model_results) in enumerate(results.items()):\n",
        "    model_scores = [model_results[metric] for metric in metrics_to_plot]\n",
        "    plt.bar(x + i*width, model_scores, width, label=model_name.replace('_', ' ').title())\n",
        "\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.xticks(x + width/2, metrics_to_plot)\n",
        "plt.legend()\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../reports/model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Visualizations saved to ../reports/ directory\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Persistence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving trained models...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'best_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Save best model separately\u001b[39;00m\n\u001b[32m     16\u001b[39m best_model_path = \u001b[33m'\u001b[39m\u001b[33m../models/best_model.pkl\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m joblib.dump(\u001b[43mbest_model\u001b[49m, best_model_path)\n\u001b[32m     18\u001b[39m saved_models[\u001b[33m'\u001b[39m\u001b[33mbest_model\u001b[39m\u001b[33m'\u001b[39m] = best_model_path\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBest model saved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'best_model' is not defined"
          ]
        }
      ],
      "source": [
        "# 7) Save models for future use\n",
        "print(\"Saving trained models...\")\n",
        "\n",
        "# Create models directory if it doesn't exist\n",
        "os.makedirs('../models', exist_ok=True)\n",
        "\n",
        "# Save all models\n",
        "saved_models = {}\n",
        "for name, model in trained_models.items():\n",
        "    model_path = f'../models/{name}.pkl'\n",
        "    joblib.dump(model, model_path)\n",
        "    saved_models[name] = model_path\n",
        "    print(f\"Saved {name}: {model_path}\")\n",
        "\n",
        "# Save best model separately\n",
        "best_model_path = '../models/best_model.pkl'\n",
        "joblib.dump(best_model, best_model_path)\n",
        "saved_models['best_model'] = best_model_path\n",
        "\n",
        "print(f\"\\nBest model saved: {best_model_path}\")\n",
        "print(f\"Best model type: {best_model_name}\")\n",
        "\n",
        "# Save model metadata\n",
        "model_info = {\n",
        "    'best_model_name': best_model_name,\n",
        "    'best_model_path': best_model_path,\n",
        "    'performance_metrics': results[best_model_name],\n",
        "    'feature_columns': list(X.columns),\n",
        "    'target_classes': [0, 1],\n",
        "    'model_files': saved_models\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('../models/model_info.json', 'w') as f:\n",
        "    json.dump(model_info, f, indent=2)\n",
        "\n",
        "print(\"Model metadata saved: ../models/model_info.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Demo Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DEMO PREDICTIONS ===\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'X_test' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 8) Demo prediction on sample data\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== DEMO PREDICTIONS ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mX_test\u001b[49m) > \u001b[32m0\u001b[39m:\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Use first test sample for demo\u001b[39;00m\n\u001b[32m      6\u001b[39m     sample_data = X_test.iloc[[\u001b[32m0\u001b[39m]]\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSample patient data:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'X_test' is not defined"
          ]
        }
      ],
      "source": [
        "# 8) Demo prediction on sample data\n",
        "print(\"=== DEMO PREDICTIONS ===\")\n",
        "\n",
        "if len(X_test) > 0:\n",
        "    # Use first test sample for demo\n",
        "    sample_data = X_test.iloc[[0]]\n",
        "    \n",
        "    print(\"Sample patient data:\")\n",
        "    for col, val in sample_data.iloc[0].items():\n",
        "        print(f\"  {col}: {val}\")\n",
        "    \n",
        "    # Make prediction\n",
        "    prediction = best_model.predict(sample_data)[0]\n",
        "    if y_proba_best is not None:\n",
        "        probability = best_model.predict_proba(sample_data)[0][1]\n",
        "        print(f\"\\nPrediction: {'Heart Disease Risk' if prediction == 1 else 'No Heart Disease Risk'}\")\n",
        "        print(f\"Probability of heart disease: {probability:.4f} ({probability*100:.1f}%)\")\n",
        "    else:\n",
        "        print(f\"\\nPrediction: {'Heart Disease Risk' if prediction == 1 else 'No Heart Disease Risk'}\")\n",
        "    \n",
        "    print(f\"Actual label: {'Heart Disease' if y_test.iloc[0] == 1 else 'No Heart Disease'}\")\n",
        "    print(f\"Prediction {'CORRECT' if prediction == y_test.iloc[0] else 'INCORRECT'}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETE! Models ready for deployment.\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
