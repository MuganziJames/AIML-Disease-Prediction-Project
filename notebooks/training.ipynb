{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Heart Disease Prediction - Model Training & Evaluation\n",
        "\n",
        "This notebook implements the complete machine learning pipeline for heart disease prediction, following the plan structure:\n",
        "\n",
        "## Table of Contents\n",
        "1. Setup & Data Loading\n",
        "2. Data Preprocessing \n",
        "3. Model Training (Logistic Regression & Random Forest)\n",
        "4. Hyperparameter Tuning\n",
        "5. Model Evaluation & Comparison\n",
        "6. Model Persistence\n",
        "7. Demo Predictions\n",
        "\n",
        "**Goal**: Build accurate models to predict heart disease risk and save the best performing model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 0) Setup - Import libraries and configure environment\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('../src')\n",
        "\n",
        "# Scikit-learn imports\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, ConfusionMatrixDisplay, \n",
        "                             RocCurveDisplay, classification_report)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import joblib\n",
        "\n",
        "# Local utilities\n",
        "from utils import load_and_create_target, plot_confusion_matrix, plot_roc_curve, print_model_metrics\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"Pandas: {pd.__version__}\")\n",
        "print(f\"NumPy: {np.__version__}\")\n",
        "print(f\"Scikit-learn imported\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading & Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Load data with target creation\n",
        "print(\"Loading heart disease dataset...\")\n",
        "df = load_and_create_target('../data/heart_dataset.csv')\n",
        "\n",
        "print(f\"Dataset loaded successfully!\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Target distribution: {df['target'].value_counts().to_dict()}\")\n",
        "\n",
        "# Separate features and target\n",
        "y = df['target'].astype(int)\n",
        "X = df.drop(columns=['target'])\n",
        "\n",
        "print(f\"\\nFeatures: {list(X.columns)}\")\n",
        "print(f\"Feature count: {X.shape[1]}\")\n",
        "print(f\"Sample count: {X.shape[0]}\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nFirst few samples:\")\n",
        "X.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Basic data cleaning and feature identification\n",
        "print(\"Data preprocessing...\")\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = X.isnull().sum().sum()\n",
        "print(f\"Missing values: {missing_values}\")\n",
        "\n",
        "# Identify feature types\n",
        "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "cat_cols = [c for c in X.columns if c not in num_cols]\n",
        "\n",
        "print(f\"Numeric features ({len(num_cols)}): {num_cols}\")\n",
        "print(f\"Categorical features ({len(cat_cols)}): {cat_cols}\")\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "# Encode string categorical columns; scale numeric columns\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), num_cols),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
        "], remainder='drop')\n",
        "\n",
        "print(\"Preprocessor created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Train/Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Train-test split\n",
        "print(\"Splitting data into train and test sets...\")\n",
        "\n",
        "# Check if we have enough data for proper split\n",
        "if len(X) >= 10:  # Need enough samples for proper stratified split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, \n",
        "        stratify=y if len(np.unique(y)) > 1 else None\n",
        "    )\n",
        "elif len(X) >= 4:  # Small dataset - no stratification\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.4, random_state=42  # Larger test size, no stratify\n",
        "    )\n",
        "    print(\"Warning: Small dataset. Using simple split without stratification.\")\n",
        "else:\n",
        "    print(\"Warning: Very small dataset. Using all data for both training and testing.\")\n",
        "    X_train, X_test = X, X\n",
        "    y_train, y_test = y, y\n",
        "\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "print(f\"Training target distribution: {y_train.value_counts().to_dict()}\")\n",
        "print(f\"Test target distribution: {y_test.value_counts().to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Baseline Models Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) Define and train baseline models\n",
        "print(\"Training baseline models...\")\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    \"logistic_regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
        "    \"random_forest\": RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "}\n",
        "\n",
        "# Store results\n",
        "results = {}\n",
        "trained_models = {}\n",
        "\n",
        "# Train each model\n",
        "for name, clf in models.items():\n",
        "    print(f\"\\nTraining {name.upper()}...\")\n",
        "    \n",
        "    # Create pipeline\n",
        "    pipe = Pipeline([\n",
        "        ('preprocessor', preprocessor), \n",
        "        ('classifier', clf)\n",
        "    ])\n",
        "    \n",
        "    # Train model\n",
        "    pipe.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = pipe.predict(X_test)\n",
        "    y_proba = pipe.predict_proba(X_test)[:, 1] if hasattr(pipe['classifier'], \"predict_proba\") else None\n",
        "    \n",
        "    # Calculate metrics (guard ROC AUC for 1-sample test sets)\n",
        "    roc_auc = None\n",
        "    if y_proba is not None and len(np.unique(y_test)) > 1 and len(y_test) >= 2:\n",
        "        roc_auc = roc_auc_score(y_test, y_proba)\n",
        "    \n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
        "        \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
        "        \"f1\": f1_score(y_test, y_pred, zero_division=0),\n",
        "        \"roc_auc\": roc_auc\n",
        "    }\n",
        "    \n",
        "    # Store results\n",
        "    results[name] = metrics\n",
        "    trained_models[name] = pipe\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"{name.upper()} Results:\")\n",
        "    print(f\"   Accuracy: {metrics['accuracy']:.4f}\")\n",
        "    print(f\"   Precision: {metrics['precision']:.4f}\")\n",
        "    print(f\"   Recall: {metrics['recall']:.4f}\")\n",
        "    print(f\"   F1 Score: {metrics['f1']:.4f}\")\n",
        "    if metrics['roc_auc'] is not None:\n",
        "        print(f\"   ROC AUC: {metrics['roc_auc']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"BASELINE MODELS TRAINING COMPLETE!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Hyperparameter Tuning (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Hyperparameter tuning for better performance\n",
        "print(\"Hyperparameter tuning (if dataset is large enough)...\")\n",
        "\n",
        "# Define parameter grids\n",
        "param_grids = {\n",
        "    'logistic_regression': {\n",
        "        'classifier__C': [0.1, 1, 10],\n",
        "        'classifier__penalty': ['l2'],\n",
        "        'classifier__solver': ['lbfgs']\n",
        "    },\n",
        "    'random_forest': {\n",
        "        'classifier__n_estimators': [100, 300],\n",
        "        'classifier__max_depth': [None, 5, 10],\n",
        "        'classifier__min_samples_split': [2, 5]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Only do hyperparameter tuning if we have enough data\n",
        "if len(X_train) > 10:\n",
        "    print(\"Performing hyperparameter tuning...\")\n",
        "    \n",
        "    tuned_models = {}\n",
        "    for name, pipe in trained_models.items():\n",
        "        if name in param_grids:\n",
        "            print(f\"\\nTuning {name.upper()}...\")\n",
        "            \n",
        "            grid_search = GridSearchCV(\n",
        "                pipe, \n",
        "                param_grids[name],\n",
        "                cv=min(3, len(X_train)//2),  # Adjust CV folds based on data size\n",
        "                scoring='roc_auc',\n",
        "                n_jobs=-1\n",
        "            )\n",
        "            \n",
        "            grid_search.fit(X_train, y_train)\n",
        "            tuned_models[name] = grid_search.best_estimator_\n",
        "            \n",
        "            print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "            print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
        "    \n",
        "    # Update models if tuning was successful\n",
        "    if tuned_models:\n",
        "        trained_models.update(tuned_models)\n",
        "        print(\"\\nModels updated with best hyperparameters!\")\n",
        "    \n",
        "else:\n",
        "    print(\"Dataset too small for hyperparameter tuning. Using default parameters.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Evaluation & Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) Comprehensive model evaluation and comparison\n",
        "print(\"=== COMPREHENSIVE MODEL EVALUATION ===\")\n",
        "\n",
        "# Create comparison DataFrame\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"\\nModel Comparison:\")\n",
        "print(results_df.round(4) if not results_df.empty else \"No results to display (very small dataset)\")\n",
        "\n",
        "if results_df.empty:\n",
        "    # Fallback: pick any trained model for downstream demo\n",
        "    best_model_name = next(iter(trained_models))\n",
        "else:\n",
        "    # Find best model\n",
        "    if 'roc_auc' in results_df.columns:\n",
        "        results_df['combined_score'] = results_df['roc_auc'].fillna(0) + results_df['f1']\n",
        "        best_model_name = results_df['combined_score'].idxmax()\n",
        "    else:\n",
        "        best_model_name = results_df['f1'].idxmax()\n",
        "\n",
        "best_model = trained_models[best_model_name]\n",
        "\n",
        "print(f\"\\nBEST MODEL: {best_model_name.upper()}\")\n",
        "if not results_df.empty:\n",
        "    print(f\"Performance: {results_df.loc[best_model_name].to_dict()}\")\n",
        "\n",
        "# Get predictions from best model\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "try:\n",
        "    y_proba_best = best_model.predict_proba(X_test)[:, 1]\n",
        "except:\n",
        "    y_proba_best = None\n",
        "\n",
        "print_model_metrics(y_test, y_pred_best, y_proba_best, best_model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6) Visualizations - Confusion Matrix and ROC Curve\n",
        "print(\"Creating visualizations...\")\n",
        "\n",
        "# Create figures directory if it doesn't exist\n",
        "os.makedirs('../reports', exist_ok=True)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "confusion_fig = plot_confusion_matrix(y_test, y_pred_best, f\"Confusion Matrix - {best_model_name.title()}\")\n",
        "plt.savefig('../reports/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC curve if probabilities available\n",
        "if y_proba_best is not None:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    roc_fig = plot_roc_curve(y_test, y_proba_best, f\"ROC Curve - {best_model_name.title()}\")\n",
        "    plt.savefig('../reports/roc_curve.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Model comparison chart\n",
        "plt.figure(figsize=(12, 8))\n",
        "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
        "x = np.arange(len(metrics_to_plot))\n",
        "width = 0.35\n",
        "\n",
        "for i, (model_name, model_results) in enumerate(results.items()):\n",
        "    model_scores = [model_results[metric] for metric in metrics_to_plot]\n",
        "    plt.bar(x + i*width, model_scores, width, label=model_name.replace('_', ' ').title())\n",
        "\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.xticks(x + width/2, metrics_to_plot)\n",
        "plt.legend()\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../reports/model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Visualizations saved to ../reports/ directory\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Persistence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7) Save models for future use\n",
        "print(\"Saving trained models...\")\n",
        "\n",
        "# Create models directory if it doesn't exist\n",
        "os.makedirs('../models', exist_ok=True)\n",
        "\n",
        "# Save all models\n",
        "saved_models = {}\n",
        "for name, model in trained_models.items():\n",
        "    model_path = f'../models/{name}.pkl'\n",
        "    joblib.dump(model, model_path)\n",
        "    saved_models[name] = model_path\n",
        "    print(f\"Saved {name}: {model_path}\")\n",
        "\n",
        "# Save best model separately\n",
        "best_model_path = '../models/best_model.pkl'\n",
        "joblib.dump(best_model, best_model_path)\n",
        "saved_models['best_model'] = best_model_path\n",
        "\n",
        "print(f\"\\nBest model saved: {best_model_path}\")\n",
        "print(f\"Best model type: {best_model_name}\")\n",
        "\n",
        "# Save model metadata\n",
        "model_info = {\n",
        "    'best_model_name': best_model_name,\n",
        "    'best_model_path': best_model_path,\n",
        "    'performance_metrics': results[best_model_name],\n",
        "    'feature_columns': list(X.columns),\n",
        "    'target_classes': [0, 1],\n",
        "    'model_files': saved_models\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('../models/model_info.json', 'w') as f:\n",
        "    json.dump(model_info, f, indent=2)\n",
        "\n",
        "print(\"Model metadata saved: ../models/model_info.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Demo Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8) Demo prediction on sample data\n",
        "print(\"=== DEMO PREDICTIONS ===\")\n",
        "\n",
        "if len(X_test) > 0:\n",
        "    # Use first test sample for demo\n",
        "    sample_data = X_test.iloc[[0]]\n",
        "    \n",
        "    print(\"Sample patient data:\")\n",
        "    for col, val in sample_data.iloc[0].items():\n",
        "        print(f\"  {col}: {val}\")\n",
        "    \n",
        "    # Make prediction\n",
        "    prediction = best_model.predict(sample_data)[0]\n",
        "    if y_proba_best is not None:\n",
        "        probability = best_model.predict_proba(sample_data)[0][1]\n",
        "        print(f\"\\nPrediction: {'Heart Disease Risk' if prediction == 1 else 'No Heart Disease Risk'}\")\n",
        "        print(f\"Probability of heart disease: {probability:.4f} ({probability*100:.1f}%)\")\n",
        "    else:\n",
        "        print(f\"\\nPrediction: {'Heart Disease Risk' if prediction == 1 else 'No Heart Disease Risk'}\")\n",
        "    \n",
        "    print(f\"Actual label: {'Heart Disease' if y_test.iloc[0] == 1 else 'No Heart Disease'}\")\n",
        "    print(f\"Prediction {'CORRECT' if prediction == y_test.iloc[0] else 'INCORRECT'}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETE! Models ready for deployment.\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
